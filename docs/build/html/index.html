
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Welcome to Echo’s documentation! &#8212; Echo 2019 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="welcome-to-echo-s-documentation">
<h1>Welcome to Echo’s documentation!<a class="headerlink" href="#welcome-to-echo-s-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<div class="contents local topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#about" id="id109">About</a><ul>
<li><a class="reference internal" href="#implemented-activation-functions" id="id110">Implemented Activation Functions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#installation" id="id111">Installation</a></li>
<li><a class="reference internal" href="#torch-examples" id="id112">Torch Examples</a><ul>
<li><a class="reference internal" href="#simple-activation-functions" id="id113">Simple Activation Functions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch-extensions-api" id="id114">PyTorch Extensions API</a><ul>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.aria2" id="id115">Echo.Activation.Torch.aria2</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.mish" id="id116">Echo.Activation.Torch.mish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.beta_mish" id="id117">Echo.Activation.Torch.beta_mish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.silu" id="id118">Echo.Activation.Torch.silu</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.eswish" id="id119">Echo.Activation.Torch.eswish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.swish" id="id120">Echo.Activation.Torch.swish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.elish" id="id121">Echo.Activation.Torch.elish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.hard_elish" id="id122">Echo.Activation.Torch.hard_elish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.mila" id="id123">Echo.Activation.Torch.mila</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.sine_relu" id="id124">Echo.Activation.Torch.sine_relu</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.fts" id="id125">Echo.Activation.Torch.fts</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.sqnl" id="id126">Echo.Activation.Torch.sqnl</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.isru" id="id127">Echo.Activation.Torch.isru</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.isrlu" id="id128">Echo.Activation.Torch.isrlu</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.bent_id" id="id129">Echo.Activation.Torch.bent_id</a></li>
<li><a class="reference internal" href="#echo-activation-torch-soft-clipping" id="id130">Echo.Activation.Torch.soft_clipping</a></li>
<li><a class="reference internal" href="#echo-activation-torch-weightedtanh" id="id131">Echo.Activation.Torch.weightedTanh</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.srelu" id="id132">Echo.Activation.Torch.srelu</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.brelu" id="id133">Echo.Activation.Torch.brelu</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.apl" id="id134">Echo.Activation.Torch.apl</a></li>
<li><a class="reference internal" href="#echo-activation-torch-soft-exponential" id="id135">Echo.Activation.Torch.soft_exponential</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.maxout" id="id136">Echo.Activation.Torch.maxout</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.functional" id="id137">Echo.Activation.Torch.functional</a></li>
</ul>
</li>
<li><a class="reference internal" href="#keras-extensions-api" id="id138">Keras Extensions API</a><ul>
<li><a class="reference internal" href="#echo-activation-keras-custom-activations" id="id139">Echo.Activation.Keras.custom_activations</a></li>
<li><a class="reference internal" href="#indices-and-tables" id="id140">Indices and tables</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="about">
<h2><a class="toc-backref" href="#id109">About</a><a class="headerlink" href="#about" title="Permalink to this headline">¶</a></h2>
<p><strong>Echo Package</strong> is created to provide an implementation of the most promising mathematical algorithms, which are missing in the most popular deep learning libraries, such as <a class="reference external" href="https://pytorch.org/">PyTorch</a>, <a class="reference external" href="https://keras.io/">Keras</a> and
<a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a>.</p>
<div class="section" id="implemented-activation-functions">
<h3><a class="toc-backref" href="#id110">Implemented Activation Functions</a><a class="headerlink" href="#implemented-activation-functions" title="Permalink to this headline">¶</a></h3>
<p>List of activation functions implemented in Echo:</p>
<ol class="arabic simple">
<li>PyTorch:</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Weighted Tanh (see <a class="reference internal" href="#module-Echo.Activation.Torch.weightedTanh" title="Echo.Activation.Torch.weightedTanh"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.weightedTanh</span></code></a>)</li>
<li>Aria2 (see <a class="reference internal" href="#module-Echo.Activation.Torch.aria2" title="Echo.Activation.Torch.aria2"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.aria2</span></code></a>)</li>
<li>SiLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.silu" title="Echo.Activation.Torch.silu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.silu</span></code></a>)</li>
<li>E-Swish (see <a class="reference internal" href="#module-Echo.Activation.Torch.eswish" title="Echo.Activation.Torch.eswish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.eswish</span></code></a>)</li>
<li>Swish (see <a class="reference internal" href="#module-Echo.Activation.Torch.swish" title="Echo.Activation.Torch.swish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.swish</span></code></a>)</li>
<li>ELiSH (see <a class="reference internal" href="#module-Echo.Activation.Torch.elish" title="Echo.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.elish</span></code></a>)</li>
<li>Hard ELiSH (see <a class="reference internal" href="#module-Echo.Activation.Torch.hard_elish" title="Echo.Activation.Torch.hard_elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.hard_elish</span></code></a>)</li>
<li>Mila (see <a class="reference internal" href="#module-Echo.Activation.Torch.mila" title="Echo.Activation.Torch.mila"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mila</span></code></a>)</li>
<li>SineReLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.sine_relu" title="Echo.Activation.Torch.sine_relu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.sine_relu</span></code></a>)</li>
<li>Flatten T-Swish (see <a class="reference internal" href="#module-Echo.Activation.Torch.fts" title="Echo.Activation.Torch.fts"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.fts</span></code></a>)</li>
<li>SQNL (see <a class="reference internal" href="#module-Echo.Activation.Torch.sqnl" title="Echo.Activation.Torch.sqnl"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.sqnl</span></code></a>)</li>
<li>Mish (see <a class="reference internal" href="#module-Echo.Activation.Torch.mish" title="Echo.Activation.Torch.mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mish</span></code></a>)</li>
<li>ISRU (see <a class="reference internal" href="#module-Echo.Activation.Torch.isru" title="Echo.Activation.Torch.isru"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.isru</span></code></a>)</li>
<li>ISRLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.isrlu" title="Echo.Activation.Torch.isrlu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.isrlu</span></code></a>)</li>
<li>Bent’s Identity (see <a class="reference internal" href="#module-Echo.Activation.Torch.bent_id" title="Echo.Activation.Torch.bent_id"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.bent_id</span></code></a>)</li>
<li>Soft Clipping (see <a class="reference internal" href="#module-Echo.Activation.Torch.soft_clipping" title="Echo.Activation.Torch.soft_clipping"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.soft_clipping</span></code></a>)</li>
<li>SReLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.srelu" title="Echo.Activation.Torch.srelu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.srelu</span></code></a>)</li>
<li>BReLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.brelu" title="Echo.Activation.Torch.brelu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.brelu</span></code></a>)</li>
<li>APL (see <a class="reference internal" href="#module-Echo.Activation.Torch.apl" title="Echo.Activation.Torch.apl"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.apl</span></code></a>)</li>
<li>Soft Exponential (see <a class="reference internal" href="#module-Echo.Activation.Torch.soft_exponential" title="Echo.Activation.Torch.soft_exponential"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.soft_exponential</span></code></a>)</li>
<li>Maxout (see <a class="reference internal" href="#module-Echo.Activation.Torch.maxout" title="Echo.Activation.Torch.maxout"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.maxout</span></code></a>)</li>
<li>Beta Mish (see <a class="reference internal" href="#module-Echo.Activation.Torch.beta_mish" title="Echo.Activation.Torch.beta_mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.beta_mish</span></code></a>)</li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li>Keras:</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Swish (see <a class="reference internal" href="#Echo.Activation.Keras.custom_activations.swish" title="Echo.Activation.Keras.custom_activations.swish"><code class="xref py py-func docutils literal notranslate"><span class="pre">Echo.Activation.Keras.custom_activations.swish()</span></code></a>)</li>
<li>ESwish (see <a class="reference internal" href="#Echo.Activation.Keras.custom_activations.eswish" title="Echo.Activation.Keras.custom_activations.eswish"><code class="xref py py-func docutils literal notranslate"><span class="pre">Echo.Activation.Keras.custom_activations.eswish()</span></code></a>)</li>
<li>ISRU (see <a class="reference internal" href="#Echo.Activation.Keras.custom_activations.isru" title="Echo.Activation.Keras.custom_activations.isru"><code class="xref py py-func docutils literal notranslate"><span class="pre">Echo.Activation.Keras.custom_activations.isru()</span></code></a>)</li>
<li>Beta Mish (see <a class="reference internal" href="#Echo.Activation.Keras.custom_activations.beta_mish" title="Echo.Activation.Keras.custom_activations.beta_mish"><code class="xref py py-func docutils literal notranslate"><span class="pre">Echo.Activation.Keras.custom_activations.beta_mish()</span></code></a>)</li>
<li>Mish (see <a class="reference internal" href="#Echo.Activation.Keras.custom_activations.mish" title="Echo.Activation.Keras.custom_activations.mish"><code class="xref py py-func docutils literal notranslate"><span class="pre">Echo.Activation.Keras.custom_activations.mish()</span></code></a>)</li>
<li>Mila (see <a class="reference internal" href="#Echo.Activation.Keras.custom_activations.mila" title="Echo.Activation.Keras.custom_activations.mila"><code class="xref py py-func docutils literal notranslate"><span class="pre">Echo.Activation.Keras.custom_activations.mila()</span></code></a>)</li>
<li>SQNL (see <a class="reference internal" href="#Echo.Activation.Keras.custom_activations.sqnl" title="Echo.Activation.Keras.custom_activations.sqnl"><code class="xref py py-func docutils literal notranslate"><span class="pre">Echo.Activation.Keras.custom_activations.sqnl()</span></code></a>)</li>
<li>Flatten T-Swish (see <a class="reference internal" href="#Echo.Activation.Keras.custom_activations.fts" title="Echo.Activation.Keras.custom_activations.fts"><code class="xref py py-func docutils literal notranslate"><span class="pre">Echo.Activation.Keras.custom_activations.fts()</span></code></a>)</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="installation">
<h2><a class="toc-backref" href="#id111">Installation</a><a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>To install Echo package follow the instructions below:</p>
<ol class="arabic simple">
<li>Clone or download <a class="reference external" href="https://github.com/digantamisra98/Echo">GitHub repository</a>.</li>
<li>Navigate to <strong>Echo</strong> folder:</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; $ cd Echo
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li>Install the package with pip:</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; $ pip install .
</pre></div>
</div>
</div>
<div class="section" id="torch-examples">
<h2><a class="toc-backref" href="#id112">Torch Examples</a><a class="headerlink" href="#torch-examples" title="Permalink to this headline">¶</a></h2>
<div class="section" id="simple-activation-functions">
<h3><a class="toc-backref" href="#id113">Simple Activation Functions</a><a class="headerlink" href="#simple-activation-functions" title="Permalink to this headline">¶</a></h3>
<p>The following code block contains an example of usage of an activation function
from Echo package:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># import activations from Echo</span>
<span class="hll"><span class="kn">from</span> <span class="nn">Echo.Activation.Torch.weightedTanh</span> <span class="kn">import</span> <span class="n">weightedTanh</span>
</span><span class="hll"><span class="kn">import</span> <span class="nn">Echo.Activation.Torch.functional</span> <span class="kn">as</span> <span class="nn">Func</span>
</span>
<span class="c1"># use activations in layers of model defined in class</span>
<span class="k">class</span> <span class="nc">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># initialize layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># make sure the input tensor is flattened</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># apply activation function from Echo</span>
<span class="hll">        <span class="n">x</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">weighted_tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Initialize the model using defined Classifier class</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>

    <span class="c1"># Create model with Sequential</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
                         <span class="p">(</span><span class="s1">&#39;fc1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span>
                         <span class="c1"># use activation function from Echo</span>
<span class="hll">                         <span class="p">(</span><span class="s1">&#39;wtahn1&#39;</span><span class="p">,</span>  <span class="n">weightedTanh</span><span class="p">(</span><span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)),</span>
</span>                         <span class="p">(</span><span class="s1">&#39;fc2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;bn2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">128</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;relu2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                         <span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;fc3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;bn3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">64</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;relu3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                         <span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;logsoftmax&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))]))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="pytorch-extensions-api">
<h2><a class="toc-backref" href="#id114">PyTorch Extensions API</a><a class="headerlink" href="#pytorch-extensions-api" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module-Echo.Activation.Torch.aria2">
<span id="echo-activation-torch-aria2"></span><h3><a class="toc-backref" href="#id115">Echo.Activation.Torch.aria2</a><a class="headerlink" href="#module-Echo.Activation.Torch.aria2" title="Permalink to this headline">¶</a></h3>
<p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>See Aria paper:
<a class="reference external" href="https://arxiv.org/abs/1805.08878">https://arxiv.org/abs/1805.08878</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.aria2.aria2">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.aria2.</code><code class="descname">aria2</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1.5</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.aria2.aria2" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/aria2.png" src="_images/aria2.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id1"><span class="problematic" id="id2">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id3"><span class="problematic" id="id4">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyper-parameter which has a two-fold effect; it reduces the curvature in 3rd quadrant as well as increases the curvature in first quadrant while lowering the value of activation (default = 1)</li>
<li>beta: the exponential growth rate (default = 0.5)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>See Aria paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/abs/1805.08878">https://arxiv.org/abs/1805.08878</a></dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">aria2</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.aria2.aria2.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.aria2.aria2.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.mish">
<span id="echo-activation-torch-mish"></span><h3><a class="toc-backref" href="#id116">Echo.Activation.Torch.mish</a><a class="headerlink" href="#module-Echo.Activation.Torch.mish" title="Permalink to this headline">¶</a></h3>
<p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.mish.mish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.mish.</code><code class="descname">mish</code><a class="headerlink" href="#Echo.Activation.Torch.mish.mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mish.png" src="_images/mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id5"><span class="problematic" id="id6">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id7"><span class="problematic" id="id8">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">mish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.mish.mish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.mish.mish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.beta_mish">
<span id="echo-activation-torch-beta-mish"></span><h3><a class="toc-backref" href="#id117">Echo.Activation.Torch.beta_mish</a><a class="headerlink" href="#module-Echo.Activation.Torch.beta_mish" title="Permalink to this headline">¶</a></h3>
<p>Applies the β mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.beta_mish.beta_mish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.beta_mish.</code><code class="descname">beta_mish</code><span class="sig-paren">(</span><em>beta=1.5</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.beta_mish.beta_mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the β mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/beta_mish.png" src="_images/beta_mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id9"><span class="problematic" id="id10">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id11"><span class="problematic" id="id12">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: hyperparameter (default = 1.5)</li>
</ul>
</dd>
<dt>References</dt>
<dd><ul class="first simple">
<li>β-Mish: An uni-parametric adaptive activation function derived from Mish:</li>
</ul>
<p class="last"><a class="reference external" href="https://github.com/digantamisra98/Beta-Mish">https://github.com/digantamisra98/Beta-Mish</a>)</p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">beta_mish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.beta_mish.beta_mish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.beta_mish.beta_mish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.silu">
<span id="echo-activation-torch-silu"></span><h3><a class="toc-backref" href="#id118">Echo.Activation.Torch.silu</a><a class="headerlink" href="#module-Echo.Activation.Torch.silu" title="Permalink to this headline">¶</a></h3>
<p>Applies the Sigmoid Linear Unit (SiLU) function element-wise:</p>
<div class="math notranslate nohighlight">
\[silu(x) = x * sigmoid(x)\]</div>
<p>See related paper:
<a class="reference external" href="https://arxiv.org/pdf/1606.08415.pdf">https://arxiv.org/pdf/1606.08415.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.silu.silu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.silu.</code><code class="descname">silu</code><a class="headerlink" href="#Echo.Activation.Torch.silu.silu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Sigmoid Linear Unit (SiLU) function element-wise:</p>
<div class="math notranslate nohighlight">
\[silu(x) = x * sigmoid(x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/silu.png" src="_images/silu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id13"><span class="problematic" id="id14">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id15"><span class="problematic" id="id16">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>Related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1606.08415.pdf">https://arxiv.org/pdf/1606.08415.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">silu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.silu.silu.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.silu.silu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.eswish">
<span id="echo-activation-torch-eswish"></span><h3><a class="toc-backref" href="#id119">Echo.Activation.Torch.eswish</a><a class="headerlink" href="#module-Echo.Activation.Torch.eswish" title="Permalink to this headline">¶</a></h3>
<p>Applies the E-Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>See E-Swish paper:
<a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.eswish.eswish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.eswish.</code><code class="descname">eswish</code><span class="sig-paren">(</span><em>beta=1.75</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.eswish.eswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the E-Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/eswish.png" src="_images/eswish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id17"><span class="problematic" id="id18">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id19"><span class="problematic" id="id20">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: a constant parameter (default value = 1.375)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">eswish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.375</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.eswish.eswish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.eswish.eswish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.swish">
<span id="echo-activation-torch-swish"></span><h3><a class="toc-backref" href="#id120">Echo.Activation.Torch.swish</a><a class="headerlink" href="#module-Echo.Activation.Torch.swish" title="Permalink to this headline">¶</a></h3>
<p>Applies the Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>See Swish paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.swish.swish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.swish.</code><code class="descname">swish</code><span class="sig-paren">(</span><em>beta=1.25</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.swish.swish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/swish.png" src="_images/swish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id21"><span class="problematic" id="id22">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id23"><span class="problematic" id="id24">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: hyperparameter, which controls the shape of the bump (default = 1.25)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Swish paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">swish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.25</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.swish.swish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.swish.swish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.elish">
<span id="echo-activation-torch-elish"></span><h3><a class="toc-backref" href="#id121">Echo.Activation.Torch.elish</a><a class="headerlink" href="#module-Echo.Activation.Torch.elish" title="Permalink to this headline">¶</a></h3>
<p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See ELiSH paper:
<a class="reference external" href="https://arxiv.org/pdf/1808.00783.pdf">https://arxiv.org/pdf/1808.00783.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.elish.elish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.elish.</code><code class="descname">elish</code><a class="headerlink" href="#Echo.Activation.Torch.elish.elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/elish.png" src="_images/elish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id25"><span class="problematic" id="id26">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id27"><span class="problematic" id="id28">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>Related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">elish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.elish.elish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.elish.elish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.hard_elish">
<span id="echo-activation-torch-hard-elish"></span><h3><a class="toc-backref" href="#id122">Echo.Activation.Torch.hard_elish</a><a class="headerlink" href="#module-Echo.Activation.Torch.hard_elish" title="Permalink to this headline">¶</a></h3>
<p>Applies the HardELiSH function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See HardELiSH paper:
<a class="reference external" href="https://arxiv.org/pdf/1808.00783.pdf">https://arxiv.org/pdf/1808.00783.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.hard_elish.hard_elish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.hard_elish.</code><code class="descname">hard_elish</code><a class="headerlink" href="#Echo.Activation.Torch.hard_elish.hard_elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardELiSH function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/hard_elish.png" src="_images/hard_elish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id29"><span class="problematic" id="id30">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id31"><span class="problematic" id="id32">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See HardELiSH paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">hard_elish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.hard_elish.hard_elish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.hard_elish.hard_elish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.mila">
<span id="echo-activation-torch-mila"></span><h3><a class="toc-backref" href="#id123">Echo.Activation.Torch.mila</a><a class="headerlink" href="#module-Echo.Activation.Torch.mila" title="Permalink to this headline">¶</a></h3>
<p>Applies the Mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x))\]</div>
<p>Refer to:
<a class="reference external" href="https://github.com/digantamisra98/Mila">https://github.com/digantamisra98/Mila</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.mila.mila">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.mila.</code><code class="descname">mila</code><span class="sig-paren">(</span><em>beta=-0.25</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.mila.mila" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mila.png" src="_images/mila.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id33"><span class="problematic" id="id34">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id35"><span class="problematic" id="id36">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: scale to control the concavity of the global minima of the function (default = -0.25)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><a class="reference external" href="https://github.com/digantamisra98/Mila">https://github.com/digantamisra98/Mila</a></li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">mila</span><span class="p">(</span><span class="n">beta</span><span class="o">=-</span><span class="mf">0.25</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.mila.mila.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.mila.mila.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.sine_relu">
<span id="echo-activation-torch-sine-relu"></span><h3><a class="toc-backref" href="#id124">Echo.Activation.Torch.sine_relu</a><a class="headerlink" href="#module-Echo.Activation.Torch.sine_relu" title="Permalink to this headline">¶</a></h3>
<p>Applies the SineReLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>See related Medium article:
<a class="reference external" href="https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d">https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.sine_relu.sine_relu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.sine_relu.</code><code class="descname">sine_relu</code><a class="headerlink" href="#Echo.Activation.Torch.sine_relu.sine_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SineReLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sine_relu.png" src="_images/sine_relu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id37"><span class="problematic" id="id38">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id39"><span class="problematic" id="id40">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related Medium article:</li>
</ul>
<p class="last"><a class="reference external" href="https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d">https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">sine_relu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.sine_relu.sine_relu.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.sine_relu.sine_relu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.fts">
<span id="echo-activation-torch-fts"></span><h3><a class="toc-backref" href="#id125">Echo.Activation.Torch.fts</a><a class="headerlink" href="#module-Echo.Activation.Torch.fts" title="Permalink to this headline">¶</a></h3>
<p>Applies the FTS (Flatten T-Swish) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See Flatten T-Swish paper:
<a class="reference external" href="https://arxiv.org/pdf/1812.06247.pdf">https://arxiv.org/pdf/1812.06247.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.fts.fts">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.fts.</code><code class="descname">fts</code><a class="headerlink" href="#Echo.Activation.Torch.fts.fts" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the FTS (Flatten T-Swish) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/fts.png" src="_images/fts.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id41"><span class="problematic" id="id42">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id43"><span class="problematic" id="id44">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Flattened T-Swish paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1812.06247.pdf">https://arxiv.org/pdf/1812.06247.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">fts</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.fts.fts.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.fts.fts.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.sqnl">
<span id="echo-activation-torch-sqnl"></span><h3><a class="toc-backref" href="#id126">Echo.Activation.Torch.sqnl</a><a class="headerlink" href="#module-Echo.Activation.Torch.sqnl" title="Permalink to this headline">¶</a></h3>
<p>Applies the SQNL function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>See SQNL paper:
<a class="reference external" href="https://ieeexplore.ieee.org/document/8489043">https://ieeexplore.ieee.org/document/8489043</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.sqnl.sqnl">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.sqnl.</code><code class="descname">sqnl</code><a class="headerlink" href="#Echo.Activation.Torch.sqnl.sqnl" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SQNL function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sqnl.png" src="_images/sqnl.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id45"><span class="problematic" id="id46">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id47"><span class="problematic" id="id48">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See SQNL paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://ieeexplore.ieee.org/document/8489043">https://ieeexplore.ieee.org/document/8489043</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">sqnl</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.sqnl.sqnl.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.sqnl.sqnl.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.isru">
<span id="echo-activation-torch-isru"></span><h3><a class="toc-backref" href="#id127">Echo.Activation.Torch.isru</a><a class="headerlink" href="#module-Echo.Activation.Torch.isru" title="Permalink to this headline">¶</a></h3>
<p>Applies the ISRU (Inverse Square Root Unit) function element-wise:</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>ISRU paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.isru.isru">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.isru.</code><code class="descname">isru</code><span class="sig-paren">(</span><em>alpha=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.isru.isru" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRU function element-wise:</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isru.png" src="_images/isru.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id49"><span class="problematic" id="id50">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id51"><span class="problematic" id="id52">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: A constant (default = 1.0)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>ISRU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">isru</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.isru.isru.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.isru.isru.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.isrlu">
<span id="echo-activation-torch-isrlu"></span><h3><a class="toc-backref" href="#id128">Echo.Activation.Torch.isrlu</a><a class="headerlink" href="#module-Echo.Activation.Torch.isrlu" title="Permalink to this headline">¶</a></h3>
<p>Applies the ISRLU (Inverse Square Root Linear Unit) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>ISRLU paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.isrlu.isrlu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.isrlu.</code><code class="descname">isrlu</code><span class="sig-paren">(</span><em>alpha=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.isrlu.isrlu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isrlu.png" src="_images/isrlu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id53"><span class="problematic" id="id54">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id55"><span class="problematic" id="id56">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyperparameter α controls the value to which an ISRLU saturates for negative inputs (default = 1)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li>ISRLU paper: <a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">isrlu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.isrlu.isrlu.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.isrlu.isrlu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.bent_id">
<span id="echo-activation-torch-bent-id"></span><h3><a class="toc-backref" href="#id129">Echo.Activation.Torch.bent_id</a><a class="headerlink" href="#module-Echo.Activation.Torch.bent_id" title="Permalink to this headline">¶</a></h3>
<p>Applies the Bent’s Identity function element-wise:</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.bent_id.bent_id">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.bent_id.</code><code class="descname">bent_id</code><a class="headerlink" href="#Echo.Activation.Torch.bent_id.bent_id" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Bent’s Identity function element-wise:</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/bent_id.png" src="_images/bent_id.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id57"><span class="problematic" id="id58">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id59"><span class="problematic" id="id60">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">bent_id</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.bent_id.bent_id.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.bent_id.bent_id.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="echo-activation-torch-soft-clipping">
<h3><a class="toc-backref" href="#id130">Echo.Activation.Torch.soft_clipping</a><a class="headerlink" href="#echo-activation-torch-soft-clipping" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-Echo.Activation.Torch.soft_clipping"></span><p>Applies Soft Clipping function element-wise:</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>See SC paper:
<a class="reference external" href="https://arxiv.org/pdf/1810.11509.pdf">https://arxiv.org/pdf/1810.11509.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.soft_clipping.soft_clipping">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.soft_clipping.</code><code class="descname">soft_clipping</code><span class="sig-paren">(</span><em>alpha=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.soft_clipping.soft_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Soft Clipping function element-wise:</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sc.png" src="_images/sc.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id61"><span class="problematic" id="id62">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id63"><span class="problematic" id="id64">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyper-parameter, which determines how close to linear the central region is and how sharply the linear region turns to the asymptotic values</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>See SC paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/pdf/1810.11509.pdf">https://arxiv.org/pdf/1810.11509.pdf</a></dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">soft_clipping</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.soft_clipping.soft_clipping.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.soft_clipping.soft_clipping.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="echo-activation-torch-weightedtanh">
<h3><a class="toc-backref" href="#id131">Echo.Activation.Torch.weightedTanh</a><a class="headerlink" href="#echo-activation-torch-weightedtanh" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-Echo.Activation.Torch.weightedTanh"></span><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.weightedTanh.weightedTanh">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.weightedTanh.</code><code class="descname">weightedTanh</code><span class="sig-paren">(</span><em>weight=1</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.weightedTanh.weightedTanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/weighted_tanh.png" src="_images/weighted_tanh.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id65"><span class="problematic" id="id66">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id67"><span class="problematic" id="id68">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>weight: hyperparameter (default = 1.0)</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weightedTanh</span><span class="p">(</span><span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.weightedTanh.weightedTanh.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.weightedTanh.weightedTanh.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.srelu">
<span id="echo-activation-torch-srelu"></span><h3><a class="toc-backref" href="#id132">Echo.Activation.Torch.srelu</a><a class="headerlink" href="#module-Echo.Activation.Torch.srelu" title="Permalink to this headline">¶</a></h3>
<p>Script defined the SReLU (S-shaped Rectified Linear Activation Unit):</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x_i) = \left\{\begin{matrix} t_i^r + a_i^r(x_i - t_i^r), x_i \geq t_i^r \\  x_i, t_i^r &gt; x_i &gt; t_i^l\\  t_i^l + a_i^l(x_i - t_i^l), x_i \leq  t_i^l \\ \end{matrix}\right.\end{split}\]</div>
<p>See SReLU paper:
<a class="reference external" href="https://arxiv.org/pdf/1512.07030.pdf">https://arxiv.org/pdf/1512.07030.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.srelu.srelu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.srelu.</code><code class="descname">srelu</code><span class="sig-paren">(</span><em>in_features</em>, <em>parameters=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/srelu.html#srelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.srelu.srelu" title="Permalink to this definition">¶</a></dt>
<dd><p>SReLU (S-shaped Rectified Linear Activation Unit): a combination of three linear functions, which perform mapping R → R with the following formulation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x_i) = \left\{\begin{matrix} t_i^r + a_i^r(x_i - t_i^r), x_i \geq t_i^r \\  x_i, t_i^r &gt; x_i &gt; t_i^l\\  t_i^l + a_i^l(x_i - t_i^l), x_i \leq  t_i^l \\ \end{matrix}\right.\end{split}\]</div>
<p>with 4 trainable parameters.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id69"><span class="problematic" id="id70">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id71"><span class="problematic" id="id72">*</span></a>), same shape as the input</li>
</ul>
</dd>
</dl>
<p>Parameters:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\{t_i^r, a_i^r, t_i^l, a_i^l\}\]</div>
</div></blockquote>
<p>4 trainable parameters, which model an individual SReLU activation unit. The subscript i indicates that we allow SReLU to vary in different channels. Parameters can be initialized manually or randomly.</p>
<dl class="docutils">
<dt>References:</dt>
<dd><ul class="first simple">
<li>See SReLU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1512.07030.pdf">https://arxiv.org/pdf/1512.07030.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">srelu_activation</span> <span class="o">=</span> <span class="n">srelu</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">srelu_activation</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.srelu.srelu.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/srelu.html#srelu.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.srelu.srelu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.brelu">
<span id="echo-activation-torch-brelu"></span><h3><a class="toc-backref" href="#id133">Echo.Activation.Torch.brelu</a><a class="headerlink" href="#module-Echo.Activation.Torch.brelu" title="Permalink to this headline">¶</a></h3>
<p>Script defined the BReLU (Bipolar Rectified Linear Activation Unit):</p>
<div class="math notranslate nohighlight">
\[\begin{split}BReLU(x_i) = \left\{\begin{matrix} f(x_i), i \mod 2 = 0\\  - f(-x_i), i \mod 2 \neq  0 \end{matrix}\right.\end{split}\]</div>
<p>See BReLU paper:
<a class="reference external" href="https://arxiv.org/pdf/1709.04054.pdf">https://arxiv.org/pdf/1709.04054.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.brelu.brelu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.brelu.</code><code class="descname">brelu</code><a class="reference internal" href="_modules/Echo/Activation/Torch/brelu.html#brelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.brelu.brelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of BReLU activation function:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}BReLU(x_i) = \left\{\begin{matrix} f(x_i), i \mod 2 = 0\\  - f(-x_i), i \mod 2 \neq  0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/brelu.png" src="_images/brelu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id73"><span class="problematic" id="id74">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id75"><span class="problematic" id="id76">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See BReLU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1709.04054.pdf">https://arxiv.org/pdf/1709.04054.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">brelu_activation</span> <span class="o">=</span> <span class="n">brelu</span><span class="o">.</span><span class="n">apply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">brelu_activation</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="staticmethod">
<dt id="Echo.Activation.Torch.brelu.brelu.backward">
<em class="property">static </em><code class="descname">backward</code><span class="sig-paren">(</span><em>ctx</em>, <em>grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/brelu.html#brelu.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.brelu.brelu.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the backward pass we receive a Tensor containing the gradient of the loss
with respect to the output, and we need to compute the gradient of the loss
with respect to the input.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="Echo.Activation.Torch.brelu.brelu.forward">
<em class="property">static </em><code class="descname">forward</code><span class="sig-paren">(</span><em>ctx</em>, <em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/brelu.html#brelu.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.brelu.brelu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the forward pass we receive a Tensor containing the input and return
a Tensor containing the output. ctx is a context object that can be used
to stash information for backward computation. You can cache arbitrary
objects for use in the backward pass using the ctx.save_for_backward method.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.apl">
<span id="echo-activation-torch-apl"></span><h3><a class="toc-backref" href="#id134">Echo.Activation.Torch.apl</a><a class="headerlink" href="#module-Echo.Activation.Torch.apl" title="Permalink to this headline">¶</a></h3>
<p>Script defined the APL (ADAPTIVE PIECEWISE LINEAR UNITS):</p>
<div class="math notranslate nohighlight">
\[APL(x_i) = max(0,x) + \sum_{s=1}^{S}{a_i^s * max(0, -x + b_i^s)}\]</div>
<p>See APL paper:
<a class="reference external" href="https://arxiv.org/pdf/1412.6830.pdf">https://arxiv.org/pdf/1412.6830.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.apl.apl">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.apl.</code><code class="descname">apl</code><span class="sig-paren">(</span><em>in_features</em>, <em>S</em>, <em>a=None</em>, <em>b=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/apl.html#apl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.apl.apl" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of APL (ADAPTIVE PIECEWISE LINEAR UNITS) unit:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[APL(x_i) = max(0,x) + \sum_{s=1}^{S}{a_i^s * max(0, -x + b_i^s)}\]</div>
</div></blockquote>
<p>with trainable parameters a and b, parameter S should be set in advance.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id77"><span class="problematic" id="id78">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id79"><span class="problematic" id="id80">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Parameters:</dt>
<dd><ul class="first last simple">
<li>S: hyperparameter, number of hinges to be set in advance</li>
<li>a: trainable parameter, control the slopes of the linear segments</li>
<li>b: trainable parameter, determine the locations of the hinges</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See APL paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1412.6830.pdf">https://arxiv.org/pdf/1412.6830.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a1</span> <span class="o">=</span> <span class="n">apl</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">a1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.apl.apl.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/apl.html#apl.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.apl.apl.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="Echo.Activation.Torch.apl.apl_function">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.apl.</code><code class="descname">apl_function</code><a class="reference internal" href="_modules/Echo/Activation/Torch/apl.html#apl_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.apl.apl_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of APL (ADAPTIVE PIECEWISE LINEAR UNITS) activation function:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[APL(x_i) = max(0,x) + \sum_{s=1}^{S}{a_i^s * max(0, -x + b_i^s)}\]</div>
</div></blockquote>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id81"><span class="problematic" id="id82">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id83"><span class="problematic" id="id84">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>a: variables control the slopes of the linear segments</li>
<li>b: variables determine the locations of the hinges</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See APL paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1412.6830.pdf">https://arxiv.org/pdf/1412.6830.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">apl_func</span> <span class="o">=</span> <span class="n">apl_function</span><span class="o">.</span><span class="n">apply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]],[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]],[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">apl_func</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="staticmethod">
<dt id="Echo.Activation.Torch.apl.apl_function.backward">
<em class="property">static </em><code class="descname">backward</code><span class="sig-paren">(</span><em>ctx</em>, <em>grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/apl.html#apl_function.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.apl.apl_function.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the backward pass we receive a Tensor containing the gradient of the loss
with respect to the output, and we need to compute the gradient of the loss
with respect to the input.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="Echo.Activation.Torch.apl.apl_function.forward">
<em class="property">static </em><code class="descname">forward</code><span class="sig-paren">(</span><em>ctx</em>, <em>input</em>, <em>a</em>, <em>b</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/apl.html#apl_function.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.apl.apl_function.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the forward pass we receive a Tensor containing the input and return
a Tensor containing the output. ctx is a context object that can be used
to stash information for backward computation. You can cache arbitrary
objects for use in the backward pass using the ctx.save_for_backward method.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="echo-activation-torch-soft-exponential">
<h3><a class="toc-backref" href="#id135">Echo.Activation.Torch.soft_exponential</a><a class="headerlink" href="#echo-activation-torch-soft-exponential" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-Echo.Activation.Torch.soft_exponential"></span><p>Script implements soft exponential activation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SoftExponential(x, \alpha) = \left\{\begin{matrix} - \frac{log(1 - \alpha(x + \alpha))}{\alpha}, \alpha &lt; 0\\  x, \alpha = 0\\  \frac{e^{\alpha * x} - 1}{\alpha} + \alpha, \alpha &gt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See related paper:
<a class="reference external" href="https://arxiv.org/pdf/1602.01321.pdf">https://arxiv.org/pdf/1602.01321.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.soft_exponential.soft_exponential">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.soft_exponential.</code><code class="descname">soft_exponential</code><span class="sig-paren">(</span><em>in_features</em>, <em>alpha=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/soft_exponential.html#soft_exponential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.soft_exponential.soft_exponential" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of soft exponential activation:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}SoftExponential(x, \alpha) = \left\{\begin{matrix} - \frac{log(1 - \alpha(x + \alpha))}{\alpha}, \alpha &lt; 0\\  x, \alpha = 0\\  \frac{e^{\alpha * x} - 1}{\alpha} + \alpha, \alpha &gt; 0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>with trainable parameter alpha.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id85"><span class="problematic" id="id86">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id87"><span class="problematic" id="id88">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Parameters:</dt>
<dd><ul class="first last simple">
<li>alpha - trainable parameter</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1602.01321.pdf">https://arxiv.org/pdf/1602.01321.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a1</span> <span class="o">=</span> <span class="n">soft_exponential</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">a1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.soft_exponential.soft_exponential.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/soft_exponential.html#soft_exponential.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.soft_exponential.soft_exponential.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.maxout">
<span id="echo-activation-torch-maxout"></span><h3><a class="toc-backref" href="#id136">Echo.Activation.Torch.maxout</a><a class="headerlink" href="#module-Echo.Activation.Torch.maxout" title="Permalink to this headline">¶</a></h3>
<p>Script implements Maxout activation</p>
<div class="math notranslate nohighlight">
\[maxout(\vec{x}) = max_i(x_i)\]</div>
<p>See related paper:
<a class="reference external" href="https://arxiv.org/pdf/1302.4389.pdf">https://arxiv.org/pdf/1302.4389.pdf</a></p>
<p>See implementation:
<a class="reference external" href="https://github.com/Usama113/Maxout-PyTorch/blob/master/Maxout.ipynb">https://github.com/Usama113/Maxout-PyTorch/blob/master/Maxout.ipynb</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.maxout.maxout">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.maxout.</code><code class="descname">maxout</code><a class="reference internal" href="_modules/Echo/Activation/Torch/maxout.html#maxout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.maxout.maxout" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of Maxout:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[maxout(\vec{x}) = max_i(x_i)\]</div>
</div></blockquote>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id89"><span class="problematic" id="id90">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id91"><span class="problematic" id="id92">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Maxout paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1302.4389.pdf">https://arxiv.org/pdf/1302.4389.pdf</a></p>
<ul class="simple">
<li>Reference to the implementation:</li>
</ul>
<p class="last"><a class="reference external" href="https://github.com/Usama113/Maxout-PyTorch/blob/master/Maxout.ipynb">https://github.com/Usama113/Maxout-PyTorch/blob/master/Maxout.ipynb</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a1</span> <span class="o">=</span> <span class="n">maxout</span><span class="o">.</span><span class="n">apply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">a1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="staticmethod">
<dt id="Echo.Activation.Torch.maxout.maxout.backward">
<em class="property">static </em><code class="descname">backward</code><span class="sig-paren">(</span><em>ctx</em>, <em>grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/maxout.html#maxout.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.maxout.maxout.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs did <a class="reference internal" href="#Echo.Activation.Torch.maxout.maxout.forward" title="Echo.Activation.Torch.maxout.maxout.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> return, and it should return as many
tensors, as there were inputs to <a class="reference internal" href="#Echo.Activation.Torch.maxout.maxout.forward" title="Echo.Activation.Torch.maxout.maxout.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the
gradient w.r.t the given output, and each returned value should be the
gradient w.r.t. the corresponding input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#Echo.Activation.Torch.maxout.maxout.backward" title="Echo.Activation.Torch.maxout.maxout.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#Echo.Activation.Torch.maxout.maxout.forward" title="Echo.Activation.Torch.maxout.maxout.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="Echo.Activation.Torch.maxout.maxout.forward">
<em class="property">static </em><code class="descname">forward</code><span class="sig-paren">(</span><em>ctx</em>, <em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/maxout.html#maxout.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.maxout.maxout.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store tensors that can be then retrieved
during the backward pass.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.functional">
<span id="echo-activation-torch-functional"></span><h3><a class="toc-backref" href="#id137">Echo.Activation.Torch.functional</a><a class="headerlink" href="#module-Echo.Activation.Torch.functional" title="Permalink to this headline">¶</a></h3>
<p>Script provides functional interface for custom activation functions.</p>
<dl class="function">
<dt id="Echo.Activation.Torch.functional.aria2">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">aria2</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1</em>, <em>alpha=1.5</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.aria2" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.aria2" title="Echo.Activation.Torch.aria2"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.aria2</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.bent_id">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">bent_id</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.bent_id" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Bent’s Identity function element-wise:</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.bent_id" title="Echo.Activation.Torch.bent_id"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.bent_id</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.beta_mish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">beta_mish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.5</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.beta_mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the β mish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.beta_mish" title="Echo.Activation.Torch.beta_mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.beta_mish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.elish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">elish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.elish" title="Echo.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.elish</span></code></a>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.elish" title="Echo.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.elish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.eswish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">eswish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.75</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.eswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the E-Swish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.eswish" title="Echo.Activation.Torch.eswish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.eswish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.fts">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">fts</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.fts" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the FTS (Flatten T-Swish) activation function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.fts" title="Echo.Activation.Torch.fts"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.fts</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.hard_elish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">hard_elish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.hard_elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.hard_elish" title="Echo.Activation.Torch.hard_elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.hard_elish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.isrlu">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">isrlu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.isrlu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.isrlu" title="Echo.Activation.Torch.isrlu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.isrlu</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.isru">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">isru</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.isru" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRU function element-wise:</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.isru" title="Echo.Activation.Torch.isru"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.isru</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.mila">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">mila</code><span class="sig-paren">(</span><em>input</em>, <em>beta=-0.25</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.mila" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(softplus(\beta + x)) = x * tanh(ln(1 + e^{\beta + x}))\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.mila" title="Echo.Activation.Torch.mila"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mila</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.mish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">mish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.mish" title="Echo.Activation.Torch.mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.silu">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">silu</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.silu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Sigmoid Linear Unit (SiLU) function element-wise:</p>
<div class="math notranslate nohighlight">
\[SiLU(x) = x * sigmoid(x)\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.silu" title="Echo.Activation.Torch.silu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.silu</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.sineReLU">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">sineReLU</code><span class="sig-paren">(</span><em>input</em>, <em>eps=0.01</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.sineReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SineReLU activation function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.sine_relu" title="Echo.Activation.Torch.sine_relu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.sine_relu</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.soft_clipping">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">soft_clipping</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.soft_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Soft Clipping function element-wise:</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.soft_clipping" title="Echo.Activation.Torch.soft_clipping"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.soft_clipping</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.sqnl">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">sqnl</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.sqnl" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SQNL activation function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.sqnl" title="Echo.Activation.Torch.sqnl"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.sqnl</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.swish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">swish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.25</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.swish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Swish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.swish" title="Echo.Activation.Torch.swish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.swish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.weighted_tanh">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">weighted_tanh</code><span class="sig-paren">(</span><em>input</em>, <em>weight=1</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Torch.functional.weighted_tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.weightedTanh" title="Echo.Activation.Torch.weightedTanh"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.weightedTanh</span></code></a>.</p>
</dd></dl>

</div>
</div>
<div class="section" id="keras-extensions-api">
<h2><a class="toc-backref" href="#id138">Keras Extensions API</a><a class="headerlink" href="#keras-extensions-api" title="Permalink to this headline">¶</a></h2>
<div class="section" id="echo-activation-keras-custom-activations">
<h3><a class="toc-backref" href="#id139">Echo.Activation.Keras.custom_activations</a><a class="headerlink" href="#echo-activation-keras-custom-activations" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-Echo.Activation.Keras.custom_activations"></span><p>Layers that act as activation functions.</p>
<dl class="class">
<dt id="Echo.Activation.Keras.custom_activations.beta_mish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Keras.custom_activations.</code><code class="descname">beta_mish</code><span class="sig-paren">(</span><em>beta=1.5</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.beta_mish" title="Permalink to this definition">¶</a></dt>
<dd><p>β mish activation function.</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/beta_mish.png" src="_images/beta_mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: A constant or a trainable parameter (default = 1.5)</li>
</ul>
</dd>
<dt>References</dt>
<dd><ul class="first simple">
<li>β-Mish: An uni-parametric adaptive activation function derived from Mish:</li>
</ul>
<p class="last"><a class="reference external" href="https://github.com/digantamisra98/Beta-Mish">https://github.com/digantamisra98/Beta-Mish</a>)</p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">beta_mish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.beta_mish.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.beta_mish.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id93"><span class="problematic" id="id94">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.beta_mish.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.beta_mish.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Shape tuple (tuple of integers)</dt>
<dd>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
</dd>
<dt># Returns</dt>
<dd>An input shape tuple.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.beta_mish.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.beta_mish.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>Python dictionary.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="Echo.Activation.Keras.custom_activations.eswish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Keras.custom_activations.</code><code class="descname">eswish</code><span class="sig-paren">(</span><em>beta=1.375</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.eswish" title="Permalink to this definition">¶</a></dt>
<dd><p>E-Swish Activation Function.</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/eswish.png" src="_images/eswish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: a constant parameter (default value = 1.375)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">eswish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.eswish.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.eswish.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id95"><span class="problematic" id="id96">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.eswish.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.eswish.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Shape tuple (tuple of integers)</dt>
<dd>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
</dd>
<dt># Returns</dt>
<dd>An input shape tuple.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.eswish.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.eswish.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>Python dictionary.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="Echo.Activation.Keras.custom_activations.fts">
<em class="property">class </em><code class="descclassname">Echo.Activation.Keras.custom_activations.</code><code class="descname">fts</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.fts" title="Permalink to this definition">¶</a></dt>
<dd><p>FTS (Flatten T-Swish) Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/fts.png" src="_images/fts.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>Flatten T-Swish paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1812.06247.pdf">https://arxiv.org/pdf/1812.06247.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">fts</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.fts.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.fts.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id97"><span class="problematic" id="id98">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.fts.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.fts.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Shape tuple (tuple of integers)</dt>
<dd>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
</dd>
<dt># Returns</dt>
<dd>An input shape tuple.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.fts.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.fts.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>Python dictionary.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="Echo.Activation.Keras.custom_activations.isru">
<em class="property">class </em><code class="descclassname">Echo.Activation.Keras.custom_activations.</code><code class="descname">isru</code><span class="sig-paren">(</span><em>alpha=1.0</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.isru" title="Permalink to this definition">¶</a></dt>
<dd><p>ISRU (Inverse Square Root Unit) Activation Function.</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isru.png" src="_images/isru.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: A constant (default = 1.0)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>ISRU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">isru</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.isru.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.isru.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id99"><span class="problematic" id="id100">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.isru.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.isru.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Shape tuple (tuple of integers)</dt>
<dd>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
</dd>
<dt># Returns</dt>
<dd>An input shape tuple.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.isru.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.isru.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>Python dictionary.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="Echo.Activation.Keras.custom_activations.mila">
<em class="property">class </em><code class="descclassname">Echo.Activation.Keras.custom_activations.</code><code class="descname">mila</code><span class="sig-paren">(</span><em>beta=-0.25</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.mila" title="Permalink to this definition">¶</a></dt>
<dd><p>Mila Activation Function.</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mila.png" src="_images/mila.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: scale to control the concavity of the global minima of the function (default = -0.25)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><a class="reference external" href="https://github.com/digantamisra98/Mila">https://github.com/digantamisra98/Mila</a></li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">mila</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.mila.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.mila.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id101"><span class="problematic" id="id102">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.mila.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.mila.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Shape tuple (tuple of integers)</dt>
<dd>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
</dd>
<dt># Returns</dt>
<dd>An input shape tuple.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.mila.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.mila.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>Python dictionary.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="Echo.Activation.Keras.custom_activations.mish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Keras.custom_activations.</code><code class="descname">mish</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Mish Activation Function.</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mish.png" src="_images/mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">mish</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.mish.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.mish.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id103"><span class="problematic" id="id104">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.mish.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.mish.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Shape tuple (tuple of integers)</dt>
<dd>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
</dd>
<dt># Returns</dt>
<dd>An input shape tuple.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.mish.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.mish.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>Python dictionary.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="Echo.Activation.Keras.custom_activations.sqnl">
<em class="property">class </em><code class="descclassname">Echo.Activation.Keras.custom_activations.</code><code class="descname">sqnl</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.sqnl" title="Permalink to this definition">¶</a></dt>
<dd><p>SQNL Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sqnl.png" src="_images/sqnl.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>SQNL Paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://ieeexplore.ieee.org/document/8489043">https://ieeexplore.ieee.org/document/8489043</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">sqnl</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.sqnl.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.sqnl.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id105"><span class="problematic" id="id106">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.sqnl.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.sqnl.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Shape tuple (tuple of integers)</dt>
<dd>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
</dd>
<dt># Returns</dt>
<dd>An input shape tuple.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.sqnl.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.sqnl.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>Python dictionary.</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="Echo.Activation.Keras.custom_activations.swish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Keras.custom_activations.</code><code class="descname">swish</code><span class="sig-paren">(</span><em>beta=1.0</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.swish" title="Permalink to this definition">¶</a></dt>
<dd><p>Swish Activation Function.</p>
<div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/swish.png" src="_images/swish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>a constant or a trainable parameter (default=1; which is equivalent to  Sigmoid-weighted Linear Unit (SiL))</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Swish paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">swish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.swish.call">
<code class="descname">call</code><span class="sig-paren">(</span><em>inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.swish.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id107"><span class="problematic" id="id108">**</span></a>kwargs: Additional keyword arguments.</dd>
<dt># Returns</dt>
<dd>A tensor or list/tuple of tensors.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.swish.compute_output_shape">
<code class="descname">compute_output_shape</code><span class="sig-paren">(</span><em>input_shape</em><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.swish.compute_output_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>Assumes that the layer will be built
to match that input shape provided.</p>
<dl class="docutils">
<dt># Arguments</dt>
<dd><dl class="first last docutils">
<dt>input_shape: Shape tuple (tuple of integers)</dt>
<dd>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
</dd>
<dt># Returns</dt>
<dd>An input shape tuple.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="Echo.Activation.Keras.custom_activations.swish.get_config">
<code class="descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#Echo.Activation.Keras.custom_activations.swish.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="docutils">
<dt># Returns</dt>
<dd>Python dictionary.</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="indices-and-tables">
<h3><a class="toc-backref" href="#id140">Indices and tables</a><a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="#">
    <img class="logo" src="_static/echo_logo.png" alt="Logo"/>
    
  </a>
</p>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=digantamisra98&repo=Echo&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Diganta Misra, Aleksandra Deis.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>